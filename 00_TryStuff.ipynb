{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#TODO: make this nicer\r\n",
    "path = \"data\\\\tox21_original\\\\tox21.sdf\"\r\n",
    "\r\n",
    "from rdkit import Chem\r\n",
    "import pandas as pd\r\n",
    "data_molecules = Chem.SDMolSupplier(path)\r\n",
    "\r\n",
    "info_file = pd.read_csv(\"data\\\\tox21_original\\\\tox21_compoundData.csv\", sep=\",\", header=0)\r\n",
    "\r\n",
    "targets = info_file.to_numpy()[:, -12:]\r\n",
    "ids = info_file[\"ID\"].to_numpy()\r\n",
    "to_del = []\r\n",
    "with open(\"file.csv\", mode = \"w\") as fi:\r\n",
    "    fi.write(\"NR-AR,NR-AR-LBD,NR-AhR,NR-Aromatase,NR-ER,NR-ER-LBD,NR-PPAR-gamma,SR-ARE,SR-ATAD5,SR-HSE,SR-MMP,SR-p53,mol_id,smiles\\n\")\r\n",
    "\r\n",
    "    for mol, target, i, infofile in zip(data_molecules, targets, ids, range(len(info_file))):\r\n",
    "        try:\r\n",
    "            smiles = Chem.MolToSmiles(mol)\r\n",
    "            #fi.write(\",\".join([str(int(t)) if str(t)!=\"nan\" else \"\" for t in target]) + \",\" + i + \",\" + smiles + \"\\n\")\r\n",
    "        except BaseException:\r\n",
    "            to_del.append(infofile)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "import pdb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "from typing import Union, Tuple, Optional\r\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\r\n",
    "                                    OptTensor)\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import Tensor\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.nn import Parameter, Linear\r\n",
    "from torch_sparse import SparseTensor, set_diag\r\n",
    "from torch_geometric.nn.conv import MessagePassing\r\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\r\n",
    "from torch_geometric.nn.inits import glorot, zeros\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class GATConv(MessagePassing):\r\n",
    "    r\"\"\"The graph attentional operator from the `\"Graph Attention Networks\"\r\n",
    "    <https://arxiv.org/abs/1710.10903>`_ paper\r\n",
    "\r\n",
    "    .. math::\r\n",
    "        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\r\n",
    "        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\r\n",
    "\r\n",
    "    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\r\n",
    "\r\n",
    "    .. math::\r\n",
    "        \\alpha_{i,j} =\r\n",
    "        \\frac{\r\n",
    "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\r\n",
    "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\r\n",
    "        \\right)\\right)}\r\n",
    "        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\r\n",
    "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\r\n",
    "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\r\n",
    "        \\right)\\right)}.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\r\n",
    "            corresponds to the sizes of source and target dimensionalities.\r\n",
    "        out_channels (int): Size of each output sample.\r\n",
    "        heads (int, optional): Number of multi-head-attentions.\r\n",
    "            (default: :obj:`1`)\r\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\r\n",
    "            attentions are averaged instead of concatenated.\r\n",
    "            (default: :obj:`True`)\r\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\r\n",
    "            slope. (default: :obj:`0.2`)\r\n",
    "        dropout (float, optional): Dropout probability of the normalized\r\n",
    "            attention coefficients which exposes each node to a stochastically\r\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\r\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\r\n",
    "            self-loops to the input graph. (default: :obj:`True`)\r\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\r\n",
    "            an additive bias. (default: :obj:`True`)\r\n",
    "        **kwargs (optional): Additional arguments of\r\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\r\n",
    "    \"\"\"\r\n",
    "    _alpha: OptTensor\r\n",
    "\r\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\r\n",
    "                 out_channels: int, heads: int = 1, concat: bool = True,\r\n",
    "                 negative_slope: float = 0.2, dropout: float = 0.0,\r\n",
    "                 add_self_loops: bool = True, bias: bool = True, **kwargs):\r\n",
    "        kwargs.setdefault('aggr', 'add')\r\n",
    "        super(GATConv, self).__init__(node_dim=0, **kwargs)\r\n",
    "\r\n",
    "        self.in_channels = in_channels\r\n",
    "        self.out_channels = out_channels\r\n",
    "        self.heads = heads\r\n",
    "        self.concat = concat\r\n",
    "        self.negative_slope = negative_slope\r\n",
    "        self.dropout = dropout\r\n",
    "        self.add_self_loops = add_self_loops\r\n",
    "\r\n",
    "        if isinstance(in_channels, int):\r\n",
    "            self.lin_l = Linear(in_channels, heads * out_channels, bias=False)\r\n",
    "            self.lin_r = self.lin_l\r\n",
    "        else:\r\n",
    "            self.lin_l = Linear(in_channels[0], heads * out_channels, False)\r\n",
    "            self.lin_r = Linear(in_channels[1], heads * out_channels, False)\r\n",
    "\r\n",
    "        self.att_l = Parameter(torch.Tensor(1, heads, out_channels))\r\n",
    "        self.att_r = Parameter(torch.Tensor(1, heads, out_channels))\r\n",
    "\r\n",
    "        if bias and concat:\r\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\r\n",
    "        elif bias and not concat:\r\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\r\n",
    "        else:\r\n",
    "            self.register_parameter('bias', None)\r\n",
    "\r\n",
    "        self._alpha = None\r\n",
    "\r\n",
    "        self.reset_parameters()\r\n",
    "\r\n",
    "    def reset_parameters(self):\r\n",
    "        glorot(self.lin_l.weight)\r\n",
    "        glorot(self.lin_r.weight)\r\n",
    "        glorot(self.att_l)\r\n",
    "        glorot(self.att_r)\r\n",
    "        zeros(self.bias)\r\n",
    "\r\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\r\n",
    "                size: Size = None, return_attention_weights=None):\r\n",
    "        # type: (Union[Tensor, OptPairTensor], Tensor, Size, NoneType) -> Tensor  # noqa\r\n",
    "        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, NoneType) -> Tensor  # noqa\r\n",
    "        # type: (Union[Tensor, OptPairTensor], Tensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\r\n",
    "        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa\r\n",
    "        r\"\"\"\r\n",
    "        Args:\r\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\r\n",
    "                will additionally return the tuple\r\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\r\n",
    "                attention weights for each edge. (default: :obj:`None`)\r\n",
    "        \"\"\"\r\n",
    "        H, C = self.heads, self.out_channels\r\n",
    "        #pdb.set_trace()\r\n",
    "\r\n",
    "        x_l: OptTensor = None\r\n",
    "        x_r: OptTensor = None\r\n",
    "        alpha_l: OptTensor = None\r\n",
    "        alpha_r: OptTensor = None\r\n",
    "        if isinstance(x, Tensor):\r\n",
    "            assert x.dim() == 2, 'Static graphs not supported in `GATConv`.'\r\n",
    "            x_l = x_r = self.lin_l(x).view(-1, H, C)\r\n",
    "            alpha_l = (x_l * self.att_l).sum(dim=-1)\r\n",
    "            alpha_r = (x_r * self.att_r).sum(dim=-1)\r\n",
    "        else:\r\n",
    "            x_l, x_r = x[0], x[1]\r\n",
    "            assert x[0].dim() == 2, 'Static graphs not supported in `GATConv`.'\r\n",
    "            x_l = self.lin_l(x_l).view(-1, H, C)\r\n",
    "            alpha_l = (x_l * self.att_l).sum(dim=-1)\r\n",
    "            if x_r is not None:\r\n",
    "                x_r = self.lin_r(x_r).view(-1, H, C)\r\n",
    "                alpha_r = (x_r * self.att_r).sum(dim=-1)\r\n",
    "\r\n",
    "        assert x_l is not None\r\n",
    "        assert alpha_l is not None\r\n",
    "\r\n",
    "        #Self loops might be that nodes are also connected to themselfes\r\n",
    "        if self.add_self_loops:\r\n",
    "            if isinstance(edge_index, Tensor):\r\n",
    "                num_nodes = x_l.size(0)\r\n",
    "                if x_r is not None:\r\n",
    "                    num_nodes = min(num_nodes, x_r.size(0))\r\n",
    "                if size is not None:\r\n",
    "                    num_nodes = min(size[0], size[1])\r\n",
    "                edge_index, _ = remove_self_loops(edge_index)\r\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\r\n",
    "            elif isinstance(edge_index, SparseTensor):\r\n",
    "                edge_index = set_diag(edge_index)\r\n",
    "\r\n",
    "        # propagate_type: (x: OptPairTensor, alpha: OptPairTensor)\r\n",
    "        print(\"x_l:\\n\",x_l)\r\n",
    "        print(\"x_r:\\n\",x_r)\r\n",
    "        print(\"alpha_l:\\n\",alpha_l)\r\n",
    "        print(\"alpha_r:\\n\", alpha_r)\r\n",
    "        print(\"size:\\n\",size)\r\n",
    "        print(\"edge_index:\\n\", edge_index)\r\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r),\r\n",
    "                             alpha=(alpha_l, alpha_r), size=size)\r\n",
    "\r\n",
    "        alpha = self._alpha\r\n",
    "        self._alpha = None\r\n",
    "\r\n",
    "        if self.concat:\r\n",
    "            out = out.view(-1, self.heads * self.out_channels)\r\n",
    "        else:\r\n",
    "            out = out.mean(dim=1)\r\n",
    "\r\n",
    "        if self.bias is not None:\r\n",
    "            out += self.bias\r\n",
    "\r\n",
    "        if isinstance(return_attention_weights, bool):\r\n",
    "            assert alpha is not None\r\n",
    "            if isinstance(edge_index, Tensor):\r\n",
    "                return out, (edge_index, alpha)\r\n",
    "            elif isinstance(edge_index, SparseTensor):\r\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\r\n",
    "        else:\r\n",
    "            return out\r\n",
    "\r\n",
    "    def message(self, x_j: Tensor, alpha_j: Tensor, alpha_i: OptTensor,\r\n",
    "                edge_index_i: Tensor, edge_index_j: Tensor, ptr: OptTensor,\r\n",
    "                size_i: Optional[int]) -> Tensor:\r\n",
    "        \"\"\"Attention construction\r\n",
    "\r\n",
    "        For the Sinkhorn-Knoff implementation the reparametization trick is used to keep differentiablity\r\n",
    "\r\n",
    "        edge_index_i: Tensor\r\n",
    "            The receiving edges of the messages\r\n",
    "        edge_index_j: Tensor\r\n",
    "            The sourcing edges of the messages\r\n",
    "        \"\"\"\r\n",
    "        #index: contains the index where each x_j and alpha goes to (where the message is sent to)\r\n",
    "        #We now need also where the message comes from to make the matrix double stochastic! (need to add it)\r\n",
    "        #This is contained in the edge_index matrix\r\n",
    "        \r\n",
    "        alpha = alpha_j if alpha_i is None else alpha_j + alpha_i\r\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\r\n",
    "        alpha = softmax(alpha, edge_index_i, ptr, size_i)\r\n",
    "        self._alpha = alpha\r\n",
    "        #alpha = F.dropout(alpha, p=self.dropout, training=self.training) #for now no dropout\r\n",
    "        print(\"alpha:\\n\", alpha)\r\n",
    "\r\n",
    "        print(\"alpha_shape:\\n\",alpha.shape)\r\n",
    "        print(\"x_j:\\n\",x_j)\r\n",
    "        print(\"x_j shape\", x_j.shape)\r\n",
    "        print(\"edge_index_i:\\n\", edge_index_i)\r\n",
    "        print(\"ptr\\n\",ptr)\r\n",
    "        print(\"size_i\\n\",size_i)\r\n",
    "        print(\"edge_index_j:\\n\",edge_index_j)\r\n",
    "        if True:\r\n",
    "            z = torch.zeros((size_i, size_i))\r\n",
    "            z[edge_index_j, edge_index_i] = alpha.squeeze()#maybe switch j and i here?\r\n",
    "            sk = SinkhornKnopp()\r\n",
    "            _ = sk.fit(z.detach().cpu().numpy())\r\n",
    "            D1 = torch.tensor(sk._D1, dtype=torch.float32)\r\n",
    "            D2 = torch.tensor(sk._D2, dtype=torch.float32)\r\n",
    "            new_z = D1 @ z @ D2\r\n",
    "            alpha = new_z[edge_index_j, edge_index_i]\r\n",
    "            alpha = alpha.unsqueeze(-1)\r\n",
    "            print(\"alpha_new:\\n\",alpha)\r\n",
    "            #TODO: reparametize here\r\n",
    "            print(\"z new:\\n\",new_z)\r\n",
    "\r\n",
    "        return x_j * alpha.unsqueeze(-1)\r\n",
    "\r\n",
    "    def __repr__(self):\r\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\r\n",
    "                                             self.in_channels,\r\n",
    "                                             self.out_channels, self.heads)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "from typing import Optional, Callable, List\r\n",
    "from torch_geometric.typing import Adj\r\n",
    "\r\n",
    "import copy\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import Tensor\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.nn import ModuleList, Sequential, Linear, BatchNorm1d, ReLU\r\n",
    "\r\n",
    "from torch_geometric.nn.models.jumping_knowledge import JumpingKnowledge\r\n",
    "\r\n",
    "from utils.basic_modules import BasicGNN\r\n",
    "\r\n",
    "class GAT(BasicGNN):\r\n",
    "    r\"\"\"The Graph Neural Network from the `\"Graph Attention Networks\"\r\n",
    "    <https://arxiv.org/abs/1710.10903>`_ paper, using the\r\n",
    "    :class:`~torch_geometric.nn.GATConv` operator for message passing.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        in_channels (int): Size of each input sample.\r\n",
    "        hidden_channels (int): Hidden node feature dimensionality.\r\n",
    "        num_layers (int): Number of GNN layers.\r\n",
    "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\r\n",
    "        act (Callable, optional): The non-linear activation function to use.\r\n",
    "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\r\n",
    "        norm (torch.nn.Module, optional): The normalization operator to use.\r\n",
    "            (default: :obj:`None`)\r\n",
    "        jk (str, optional): The Jumping Knowledge mode\r\n",
    "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\r\n",
    "            (default: :obj:`\"last\"`)\r\n",
    "        **kwargs (optional): Additional arguments of\r\n",
    "            :class:`torch_geometric.nn.conv.GATConv`.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\r\n",
    "                 dropout: float = 0.0,\r\n",
    "                 act: Optional[Callable] = ReLU(inplace=True),\r\n",
    "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\r\n",
    "                 **kwargs):\r\n",
    "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\r\n",
    "                         act, norm, jk)\r\n",
    "\r\n",
    "        if 'concat' in kwargs:\r\n",
    "            del kwargs['concat']\r\n",
    "\r\n",
    "        if 'heads' in kwargs:\r\n",
    "            assert hidden_channels % kwargs['heads'] == 0\r\n",
    "        out_channels = hidden_channels // kwargs.get('heads', 1)\r\n",
    "\r\n",
    "        self.convs.append(\r\n",
    "            GATConv(in_channels, out_channels, dropout=dropout, **kwargs))\r\n",
    "        for _ in range(1, num_layers):\r\n",
    "            self.convs.append(GATConv(hidden_channels, out_channels, **kwargs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "source": [
    "gat = GAT(2, 3, 1, add_self_loops = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "source": [
    "#data = torch.ones((5, 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "source": [
    "#adj_data = torch.tensor([[1,2,3,1],[2,1,1,3]],dtype=torch.long)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "source": [
    "gat(data, adj_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_l:\n",
      " tensor([[[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]]], grad_fn=<ViewBackward>)\n",
      "x_r:\n",
      " tensor([[[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]]], grad_fn=<ViewBackward>)\n",
      "alpha_l:\n",
      " tensor([[-0.7814],\n",
      "        [-0.7814],\n",
      "        [-0.7814],\n",
      "        [-0.7814],\n",
      "        [-0.7814]], grad_fn=<SumBackward1>)\n",
      "alpha_r:\n",
      " tensor([[0.1232],\n",
      "        [0.1232],\n",
      "        [0.1232],\n",
      "        [0.1232],\n",
      "        [0.1232]], grad_fn=<SumBackward1>)\n",
      "size:\n",
      " None\n",
      "edge_index:\n",
      " tensor([[1, 2, 3, 1, 0, 1, 2, 3, 4],\n",
      "        [2, 1, 1, 3, 0, 1, 2, 3, 4]])\n",
      "alpha:\n",
      " tensor([[0.5000],\n",
      "        [0.3333],\n",
      "        [0.3333],\n",
      "        [0.5000],\n",
      "        [1.0000],\n",
      "        [0.3333],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [1.0000]], grad_fn=<DifferentiableGraphBackward>)\n",
      "alpha_shape:\n",
      " torch.Size([9, 1])\n",
      "x_j:\n",
      " tensor([[[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]],\n",
      "\n",
      "        [[ 0.1211,  1.7863, -1.0226]]], grad_fn=<IndexSelectBackward>)\n",
      "x_j shape torch.Size([9, 1, 3])\n",
      "edge_index_i:\n",
      " tensor([2, 1, 1, 3, 0, 1, 2, 3, 4])\n",
      "ptr\n",
      " None\n",
      "size_i\n",
      " 5\n",
      "edge_index_j:\n",
      " tensor([1, 2, 3, 1, 0, 1, 2, 3, 4])\n",
      "alpha_new:\n",
      " tensor([[0.3820],\n",
      "        [0.3820],\n",
      "        [0.3820],\n",
      "        [0.3820],\n",
      "        [1.0000],\n",
      "        [0.2361],\n",
      "        [0.6180],\n",
      "        [0.6180],\n",
      "        [1.0000]], grad_fn=<UnsqueezeBackward0>)\n",
      "z new:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2361, 0.3820, 0.3820, 0.0000],\n",
      "        [0.0000, 0.3820, 0.6180, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3820, 0.0000, 0.6180, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]], grad_fn=<MmBackward>)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1211, 1.7863, 0.0000],\n",
       "        [0.1211, 1.7864, 0.0000],\n",
       "        [0.1211, 1.7863, 0.0000],\n",
       "        [0.1211, 1.7863, 0.0000],\n",
       "        [0.1211, 1.7863, 0.0000]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 230
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Try Sinkhorn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from main import main"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "main(logdir=\"sinkhorn_test2\", architecture=\"sinkhorn\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of Grid-Search configurations: 288\n",
      "Number of configurations now being trained 5\n",
      "--------------------------------------------------------------------------------------------\n",
      "\n",
      "Training config _hiddenchannels-256_headdepth-4_basedepth-5_basedropout-0.2_headdropout-0.2_lr-0.01 ... "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sinkhorn Pytorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def sinkhorn(P: Tensor, threshhold = 1e-3, max_iter = 100):\n",
    "    \"\"\"Fit the diagonal matrices in Sinkhorn Knopp's algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : 2d Tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A double stochastic matrix.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    N = P.shape[0]\n",
    "    max_thresh = 1 + threshhold\n",
    "    min_thresh = 1 - threshhold\n",
    "\n",
    "    # Initialize r and c, the diagonals of D1 and D2\n",
    "    r = torch.ones(N)\n",
    "    pdotr = P.T @ r\n",
    "\n",
    "    c = 1 / pdotr\n",
    "    pdotc = P @ c\n",
    "\n",
    "\n",
    "    r = 1 / pdotc\n",
    "    del pdotr, pdotc\n",
    "    P_eps = copy.copy(P)\n",
    "\n",
    "    iterations = 0\n",
    "    stopping_condition = None\n",
    "    while torch.any(torch.sum(P_eps, dim=1) < min_thresh) \\\n",
    "            or torch.any(torch.sum(P_eps, dim=1) > max_thresh) \\\n",
    "            or torch.any(torch.sum(P_eps, dim=0) < min_thresh) \\\n",
    "            or torch.any(torch.sum(P_eps, dim=0) > max_thresh):\n",
    "\n",
    "        c = 1 / (P.T @ r)\n",
    "        r = 1 / (P @ c)\n",
    "\n",
    "        P_eps = ((P * c).T * r).T\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "        if iterations >= max_iter:\n",
    "            stopping_condition = \"max_iter\"\n",
    "            break\n",
    "\n",
    "    if not stopping_condition:\n",
    "        stopping_condition = \"epsilon\"\n",
    "\n",
    "\n",
    "    P_eps = ((P * c).T * r).T\n",
    "\n",
    "\n",
    "    return P_eps, r, c, stopping_condition"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "ten = torch.from_numpy(np.random.uniform(0.1, 1, (3,3))).float()\n",
    "ten[0,1] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "tw = torch.sparse_coo_tensor(indices = torch.tensor([[0,1,1,2], [0, 1, 2, 2]]), values = torch.tensor([0.1,0.5,0.7,0.9]))._to_sparse_csr()\n",
    "tw.to_dense()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.5000, 0.7000],\n",
       "        [0.0000, 0.0000, 0.9000]])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "P_eps, r, c, stopping_condition = sinkhorn(ten)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "ten"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2122, 0.0000, 0.1146],\n",
       "        [0.2251, 0.4715, 0.6519],\n",
       "        [0.2245, 0.2173, 0.7087]])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "P_eps"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.7041, 0.0000, 0.2959],\n",
       "        [0.1223, 0.6022, 0.2755],\n",
       "        [0.1745, 0.3970, 0.4285]])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "stopping_condition"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'epsilon'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('ohg': conda)"
  },
  "interpreter": {
   "hash": "70fe7228592cc26bed22485915261bc09ab237a426fde32fcd3cbc0f918f2ec6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}