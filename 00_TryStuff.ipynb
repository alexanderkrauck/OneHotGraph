{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#TODO: make this nicer\r\n",
    "path = \"data\\\\tox21_original\\\\tox21.sdf\"\r\n",
    "\r\n",
    "from rdkit import Chem\r\n",
    "import pandas as pd\r\n",
    "data_molecules = Chem.SDMolSupplier(path)\r\n",
    "\r\n",
    "info_file = pd.read_csv(\"data\\\\tox21_original\\\\tox21_compoundData.csv\", sep=\",\", header=0)\r\n",
    "\r\n",
    "targets = info_file.to_numpy()[:, -12:]\r\n",
    "ids = info_file[\"ID\"].to_numpy()\r\n",
    "to_del = []\r\n",
    "with open(\"file.csv\", mode = \"w\") as fi:\r\n",
    "    fi.write(\"NR-AR,NR-AR-LBD,NR-AhR,NR-Aromatase,NR-ER,NR-ER-LBD,NR-PPAR-gamma,SR-ARE,SR-ATAD5,SR-HSE,SR-MMP,SR-p53,mol_id,smiles\\n\")\r\n",
    "\r\n",
    "    for mol, target, i, infofile in zip(data_molecules, targets, ids, range(len(info_file))):\r\n",
    "        try:\r\n",
    "            smiles = Chem.MolToSmiles(mol)\r\n",
    "            #fi.write(\",\".join([str(int(t)) if str(t)!=\"nan\" else \"\" for t in target]) + \",\" + i + \",\" + smiles + \"\\n\")\r\n",
    "        except BaseException:\r\n",
    "            to_del.append(infofile)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "import pdb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "source": [
    "from typing import Union, Tuple, Optional\r\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\r\n",
    "                                    OptTensor)\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import Tensor\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.nn import Parameter, Linear\r\n",
    "from torch_sparse import SparseTensor, set_diag\r\n",
    "from torch_geometric.nn.conv import MessagePassing\r\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\r\n",
    "from torch_geometric.nn.inits import glorot, zeros\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class GATConv(MessagePassing):\r\n",
    "    r\"\"\"The graph attentional operator from the `\"Graph Attention Networks\"\r\n",
    "    <https://arxiv.org/abs/1710.10903>`_ paper\r\n",
    "\r\n",
    "    .. math::\r\n",
    "        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\r\n",
    "        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\r\n",
    "\r\n",
    "    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\r\n",
    "\r\n",
    "    .. math::\r\n",
    "        \\alpha_{i,j} =\r\n",
    "        \\frac{\r\n",
    "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\r\n",
    "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\r\n",
    "        \\right)\\right)}\r\n",
    "        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\r\n",
    "        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\r\n",
    "        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\r\n",
    "        \\right)\\right)}.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\r\n",
    "            corresponds to the sizes of source and target dimensionalities.\r\n",
    "        out_channels (int): Size of each output sample.\r\n",
    "        heads (int, optional): Number of multi-head-attentions.\r\n",
    "            (default: :obj:`1`)\r\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\r\n",
    "            attentions are averaged instead of concatenated.\r\n",
    "            (default: :obj:`True`)\r\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\r\n",
    "            slope. (default: :obj:`0.2`)\r\n",
    "        dropout (float, optional): Dropout probability of the normalized\r\n",
    "            attention coefficients which exposes each node to a stochastically\r\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\r\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\r\n",
    "            self-loops to the input graph. (default: :obj:`True`)\r\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\r\n",
    "            an additive bias. (default: :obj:`True`)\r\n",
    "        **kwargs (optional): Additional arguments of\r\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\r\n",
    "    \"\"\"\r\n",
    "    _alpha: OptTensor\r\n",
    "\r\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\r\n",
    "                 out_channels: int, heads: int = 1, concat: bool = True,\r\n",
    "                 negative_slope: float = 0.2, dropout: float = 0.0,\r\n",
    "                 add_self_loops: bool = True, bias: bool = True, **kwargs):\r\n",
    "        kwargs.setdefault('aggr', 'add')\r\n",
    "        super(GATConv, self).__init__(node_dim=0, **kwargs)\r\n",
    "\r\n",
    "        self.in_channels = in_channels\r\n",
    "        self.out_channels = out_channels\r\n",
    "        self.heads = heads\r\n",
    "        self.concat = concat\r\n",
    "        self.negative_slope = negative_slope\r\n",
    "        self.dropout = dropout\r\n",
    "        self.add_self_loops = add_self_loops\r\n",
    "\r\n",
    "        if isinstance(in_channels, int):\r\n",
    "            self.lin_l = Linear(in_channels, heads * out_channels, bias=False)\r\n",
    "            self.lin_r = self.lin_l\r\n",
    "        else:\r\n",
    "            self.lin_l = Linear(in_channels[0], heads * out_channels, False)\r\n",
    "            self.lin_r = Linear(in_channels[1], heads * out_channels, False)\r\n",
    "\r\n",
    "        self.att_l = Parameter(torch.Tensor(1, heads, out_channels))\r\n",
    "        self.att_r = Parameter(torch.Tensor(1, heads, out_channels))\r\n",
    "\r\n",
    "        if bias and concat:\r\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\r\n",
    "        elif bias and not concat:\r\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\r\n",
    "        else:\r\n",
    "            self.register_parameter('bias', None)\r\n",
    "\r\n",
    "        self._alpha = None\r\n",
    "\r\n",
    "        self.reset_parameters()\r\n",
    "\r\n",
    "    def reset_parameters(self):\r\n",
    "        glorot(self.lin_l.weight)\r\n",
    "        glorot(self.lin_r.weight)\r\n",
    "        glorot(self.att_l)\r\n",
    "        glorot(self.att_r)\r\n",
    "        zeros(self.bias)\r\n",
    "\r\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\r\n",
    "                size: Size = None, return_attention_weights=None):\r\n",
    "        # type: (Union[Tensor, OptPairTensor], Tensor, Size, NoneType) -> Tensor  # noqa\r\n",
    "        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, NoneType) -> Tensor  # noqa\r\n",
    "        # type: (Union[Tensor, OptPairTensor], Tensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\r\n",
    "        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa\r\n",
    "        r\"\"\"\r\n",
    "        Args:\r\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\r\n",
    "                will additionally return the tuple\r\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\r\n",
    "                attention weights for each edge. (default: :obj:`None`)\r\n",
    "        \"\"\"\r\n",
    "        H, C = self.heads, self.out_channels\r\n",
    "        #pdb.set_trace()\r\n",
    "\r\n",
    "        x_l: OptTensor = None\r\n",
    "        x_r: OptTensor = None\r\n",
    "        alpha_l: OptTensor = None\r\n",
    "        alpha_r: OptTensor = None\r\n",
    "        if isinstance(x, Tensor):\r\n",
    "            assert x.dim() == 2, 'Static graphs not supported in `GATConv`.'\r\n",
    "            x_l = x_r = self.lin_l(x).view(-1, H, C)\r\n",
    "            alpha_l = (x_l * self.att_l).sum(dim=-1)\r\n",
    "            alpha_r = (x_r * self.att_r).sum(dim=-1)\r\n",
    "        else:\r\n",
    "            x_l, x_r = x[0], x[1]\r\n",
    "            assert x[0].dim() == 2, 'Static graphs not supported in `GATConv`.'\r\n",
    "            x_l = self.lin_l(x_l).view(-1, H, C)\r\n",
    "            alpha_l = (x_l * self.att_l).sum(dim=-1)\r\n",
    "            if x_r is not None:\r\n",
    "                x_r = self.lin_r(x_r).view(-1, H, C)\r\n",
    "                alpha_r = (x_r * self.att_r).sum(dim=-1)\r\n",
    "\r\n",
    "        assert x_l is not None\r\n",
    "        assert alpha_l is not None\r\n",
    "\r\n",
    "        #Self loops might be that nodes are also connected to themselfes\r\n",
    "        if self.add_self_loops:\r\n",
    "            if isinstance(edge_index, Tensor):\r\n",
    "                num_nodes = x_l.size(0)\r\n",
    "                if x_r is not None:\r\n",
    "                    num_nodes = min(num_nodes, x_r.size(0))\r\n",
    "                if size is not None:\r\n",
    "                    num_nodes = min(size[0], size[1])\r\n",
    "                edge_index, _ = remove_self_loops(edge_index)\r\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\r\n",
    "            elif isinstance(edge_index, SparseTensor):\r\n",
    "                edge_index = set_diag(edge_index)\r\n",
    "\r\n",
    "        # propagate_type: (x: OptPairTensor, alpha: OptPairTensor)\r\n",
    "        print(\"x_l:\\n\",x_l)\r\n",
    "        print(\"x_r:\\n\",x_r)\r\n",
    "        print(\"alpha_l:\\n\",alpha_l)\r\n",
    "        print(\"alpha_r:\\n\", alpha_r)\r\n",
    "        print(\"size:\\n\",size)\r\n",
    "        print(\"edge_index:\\n\", edge_index)\r\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r),\r\n",
    "                             alpha=(alpha_l, alpha_r), size=size)\r\n",
    "\r\n",
    "        alpha = self._alpha\r\n",
    "        self._alpha = None\r\n",
    "\r\n",
    "        if self.concat:\r\n",
    "            out = out.view(-1, self.heads * self.out_channels)\r\n",
    "        else:\r\n",
    "            out = out.mean(dim=1)\r\n",
    "\r\n",
    "        if self.bias is not None:\r\n",
    "            out += self.bias\r\n",
    "\r\n",
    "        if isinstance(return_attention_weights, bool):\r\n",
    "            assert alpha is not None\r\n",
    "            if isinstance(edge_index, Tensor):\r\n",
    "                return out, (edge_index, alpha)\r\n",
    "            elif isinstance(edge_index, SparseTensor):\r\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\r\n",
    "        else:\r\n",
    "            return out\r\n",
    "\r\n",
    "    def message(self, x_j: Tensor, alpha_j: Tensor, alpha_i: OptTensor,\r\n",
    "                edge_index_i: Tensor, edge_index_j: Tensor, ptr: OptTensor,\r\n",
    "                size_i: Optional[int]) -> Tensor:\r\n",
    "        \"\"\"Attention construction\r\n",
    "\r\n",
    "        For the Sinkhorn-Knoff implementation the reparametization trick is used to keep differentiablity\r\n",
    "\r\n",
    "        edge_index_i: Tensor\r\n",
    "            The receiving edges of the messages\r\n",
    "        edge_index_j: Tensor\r\n",
    "            The sourcing edges of the messages\r\n",
    "        \"\"\"\r\n",
    "        #index: contains the index where each x_j and alpha goes to (where the message is sent to)\r\n",
    "        #We now need also where the message comes from to make the matrix double stochastic! (need to add it)\r\n",
    "        #This is contained in the edge_index matrix\r\n",
    "        \r\n",
    "        alpha = alpha_j if alpha_i is None else alpha_j + alpha_i\r\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\r\n",
    "        alpha = softmax(alpha, edge_index_i, ptr, size_i)\r\n",
    "        self._alpha = alpha\r\n",
    "        #alpha = F.dropout(alpha, p=self.dropout, training=self.training) #for now no dropout\r\n",
    "        print(\"alpha:\\n\", alpha)\r\n",
    "\r\n",
    "        print(\"alpha_shape:\\n\",alpha.shape)\r\n",
    "        print(\"x_j:\\n\",x_j)\r\n",
    "        print(\"x_j shape\", x_j.shape)\r\n",
    "        print(\"edge_index_i:\\n\", edge_index_i)\r\n",
    "        print(\"ptr\\n\",ptr)\r\n",
    "        print(\"size_i\\n\",size_i)\r\n",
    "        print(\"edge_index_j:\\n\",edge_index_j)\r\n",
    "        if True:\r\n",
    "            z = torch.zeros((size_i, size_i))\r\n",
    "            z[edge_index_j, edge_index_i] = alpha.squeeze()#maybe switch j and i here?\r\n",
    "            sk = SinkhornKnopp()\r\n",
    "            _ = sk.fit(z.detach().cpu().numpy())\r\n",
    "            D1 = torch.tensor(sk._D1, dtype=torch.float32)\r\n",
    "            D2 = torch.tensor(sk._D2, dtype=torch.float32)\r\n",
    "            new_z = D1 @ z @ D2\r\n",
    "            alpha = new_z[edge_index_j, edge_index_i]\r\n",
    "            alpha = alpha.unsqueeze(-1)\r\n",
    "            print(\"alpha_new:\\n\",alpha)\r\n",
    "            #TODO: reparametize here\r\n",
    "            print(\"z new:\\n\",new_z)\r\n",
    "\r\n",
    "        return x_j * alpha.unsqueeze(-1)\r\n",
    "\r\n",
    "    def __repr__(self):\r\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\r\n",
    "                                             self.in_channels,\r\n",
    "                                             self.out_channels, self.heads)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "source": [
    "from typing import Optional, Callable, List\r\n",
    "from torch_geometric.typing import Adj\r\n",
    "\r\n",
    "import copy\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import Tensor\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.nn import ModuleList, Sequential, Linear, BatchNorm1d, ReLU\r\n",
    "\r\n",
    "from torch_geometric.nn.models.jumping_knowledge import JumpingKnowledge\r\n",
    "\r\n",
    "from utils.basic_modules import BasicGNN\r\n",
    "\r\n",
    "class GAT(BasicGNN):\r\n",
    "    r\"\"\"The Graph Neural Network from the `\"Graph Attention Networks\"\r\n",
    "    <https://arxiv.org/abs/1710.10903>`_ paper, using the\r\n",
    "    :class:`~torch_geometric.nn.GATConv` operator for message passing.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        in_channels (int): Size of each input sample.\r\n",
    "        hidden_channels (int): Hidden node feature dimensionality.\r\n",
    "        num_layers (int): Number of GNN layers.\r\n",
    "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\r\n",
    "        act (Callable, optional): The non-linear activation function to use.\r\n",
    "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\r\n",
    "        norm (torch.nn.Module, optional): The normalization operator to use.\r\n",
    "            (default: :obj:`None`)\r\n",
    "        jk (str, optional): The Jumping Knowledge mode\r\n",
    "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\r\n",
    "            (default: :obj:`\"last\"`)\r\n",
    "        **kwargs (optional): Additional arguments of\r\n",
    "            :class:`torch_geometric.nn.conv.GATConv`.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\r\n",
    "                 dropout: float = 0.0,\r\n",
    "                 act: Optional[Callable] = ReLU(inplace=True),\r\n",
    "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\r\n",
    "                 **kwargs):\r\n",
    "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\r\n",
    "                         act, norm, jk)\r\n",
    "\r\n",
    "        if 'concat' in kwargs:\r\n",
    "            del kwargs['concat']\r\n",
    "\r\n",
    "        if 'heads' in kwargs:\r\n",
    "            assert hidden_channels % kwargs['heads'] == 0\r\n",
    "        out_channels = hidden_channels // kwargs.get('heads', 1)\r\n",
    "\r\n",
    "        self.convs.append(\r\n",
    "            GATConv(in_channels, out_channels, dropout=dropout, **kwargs))\r\n",
    "        for _ in range(1, num_layers):\r\n",
    "            self.convs.append(GATConv(hidden_channels, out_channels, **kwargs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "source": [
    "gat = GAT(2, 3, 1, add_self_loops = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "#data = torch.ones((5, 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "source": [
    "#adj_data = torch.tensor([[1,2,3,1],[2,1,1,3]],dtype=torch.long)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "gat(data, adj_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_l:\n",
      " tensor([[[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]]], grad_fn=<ViewBackward>)\n",
      "x_r:\n",
      " tensor([[[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]]], grad_fn=<ViewBackward>)\n",
      "alpha_l:\n",
      " tensor([[-0.0963],\n",
      "        [-0.0963],\n",
      "        [-0.0963],\n",
      "        [-0.0963],\n",
      "        [-0.0963]], grad_fn=<SumBackward1>)\n",
      "alpha_r:\n",
      " tensor([[0.1376],\n",
      "        [0.1376],\n",
      "        [0.1376],\n",
      "        [0.1376],\n",
      "        [0.1376]], grad_fn=<SumBackward1>)\n",
      "size:\n",
      " None\n",
      "edge_index:\n",
      " tensor([[1, 2, 3, 1, 0, 1, 2, 3, 4],\n",
      "        [2, 1, 1, 3, 0, 1, 2, 3, 4]])\n",
      "alpha:\n",
      " tensor([[0.5000],\n",
      "        [0.3333],\n",
      "        [0.3333],\n",
      "        [0.5000],\n",
      "        [1.0000],\n",
      "        [0.3333],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [1.0000]], grad_fn=<DifferentiableGraphBackward>)\n",
      "alpha_shape:\n",
      " torch.Size([9, 1])\n",
      "x_j:\n",
      " tensor([[[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]],\n",
      "\n",
      "        [[-1.3076,  0.9852, -0.9541]]], grad_fn=<IndexSelectBackward>)\n",
      "x_j shape torch.Size([9, 1, 3])\n",
      "edge_index_i:\n",
      " tensor([2, 1, 1, 3, 0, 1, 2, 3, 4])\n",
      "ptr\n",
      " None\n",
      "size_i\n",
      " 5\n",
      "edge_index_j:\n",
      " tensor([1, 2, 3, 1, 0, 1, 2, 3, 4])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.9852, 0.0000],\n",
       "        [0.0000, 0.9852, 0.0000],\n",
       "        [0.0000, 0.9852, 0.0000],\n",
       "        [0.0000, 0.9852, 0.0000],\n",
       "        [0.0000, 0.9852, 0.0000]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 220
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "#https://github.com/btaba/sinkhorn_knopp\r\n",
    "\r\n",
    "import warnings\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "class SinkhornKnopp:\r\n",
    "    \"\"\"\r\n",
    "    Sinkhorn Knopp Algorithm\r\n",
    "\r\n",
    "    Takes a non-negative square matrix P, where P =/= 0\r\n",
    "    and iterates through Sinkhorn Knopp's algorithm\r\n",
    "    to convert P to a doubly stochastic matrix.\r\n",
    "    Guaranteed convergence if P has total support.\r\n",
    "\r\n",
    "    For reference see original paper:\r\n",
    "        http://msp.org/pjm/1967/21-2/pjm-v21-n2-p14-s.pdf\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    max_iter : int, default=1000\r\n",
    "        The maximum number of iterations.\r\n",
    "\r\n",
    "    epsilon : float, default=1e-3\r\n",
    "        Metric used to compute the stopping condition,\r\n",
    "        which occurs if all the row and column sums are\r\n",
    "        within epsilon of 1. This should be a very small value.\r\n",
    "        Epsilon must be between 0 and 1.\r\n",
    "\r\n",
    "    Attributes\r\n",
    "    ----------\r\n",
    "    _max_iter : int, default=1000\r\n",
    "        User defined parameter. See above.\r\n",
    "\r\n",
    "    _epsilon : float, default=1e-3\r\n",
    "        User defined paramter. See above.\r\n",
    "\r\n",
    "    _stopping_condition: string\r\n",
    "        Either \"max_iter\", \"epsilon\", or None, which is a\r\n",
    "        description of why the algorithm stopped iterating.\r\n",
    "\r\n",
    "    _iterations : int\r\n",
    "        The number of iterations elapsed during the algorithm's\r\n",
    "        run-time.\r\n",
    "\r\n",
    "    _D1 : 2d-array\r\n",
    "        Diagonal matrix obtained after a stopping condition was met\r\n",
    "        so that _D1.dot(P).dot(_D2) is close to doubly stochastic.\r\n",
    "\r\n",
    "    _D2 : 2d-array\r\n",
    "        Diagonal matrix obtained after a stopping condition was met\r\n",
    "        so that _D1.dot(P).dot(_D2) is close to doubly stochastic.\r\n",
    "\r\n",
    "    Example\r\n",
    "    -------\r\n",
    "\r\n",
    "    .. code-block:: python\r\n",
    "        >>> import numpy as np\r\n",
    "        >>> from sinkhorn_knopp import sinkhorn_knopp as skp\r\n",
    "        >>> sk = skp.SinkhornKnopp()\r\n",
    "        >>> P = [[.011, .15], [1.71, .1]]\r\n",
    "        >>> P_ds = sk.fit(P)\r\n",
    "        >>> P_ds\r\n",
    "        array([[ 0.06102561,  0.93897439],\r\n",
    "           [ 0.93809928,  0.06190072]])\r\n",
    "        >>> np.sum(P_ds, axis=0)\r\n",
    "        array([ 0.99912489,  1.00087511])\r\n",
    "        >>> np.sum(P_ds, axis=1)\r\n",
    "        array([ 1.,  1.])\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, max_iter=1000, epsilon=1e-3):\r\n",
    "        assert isinstance(max_iter, int) or isinstance(max_iter, float),\\\r\n",
    "            \"max_iter is not of type int or float: %r\" % max_iter\r\n",
    "        assert max_iter > 0,\\\r\n",
    "            \"max_iter must be greater than 0: %r\" % max_iter\r\n",
    "        self._max_iter = int(max_iter)\r\n",
    "\r\n",
    "        assert isinstance(epsilon, int) or isinstance(epsilon, float),\\\r\n",
    "            \"epsilon is not of type float or int: %r\" % epsilon\r\n",
    "        assert epsilon > 0 and epsilon < 1,\\\r\n",
    "            \"epsilon must be between 0 and 1 exclusive: %r\" % epsilon\r\n",
    "        self._epsilon = epsilon\r\n",
    "\r\n",
    "        self._stopping_condition = None\r\n",
    "        self._iterations = 0\r\n",
    "        self._D1 = np.ones(1)\r\n",
    "        self._D2 = np.ones(1)\r\n",
    "\r\n",
    "    def fit(self, P):\r\n",
    "        \"\"\"Fit the diagonal matrices in Sinkhorn Knopp's algorithm\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        P : 2d array-like\r\n",
    "        Must be a square non-negative 2d array-like object, that\r\n",
    "        is convertible to a numpy array. The matrix must not be\r\n",
    "        equal to 0 and it must have total support for the algorithm\r\n",
    "        to converge.\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        A double stochastic matrix.\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        P = np.asarray(P)\r\n",
    "        assert np.all(P >= 0)\r\n",
    "        assert P.ndim == 2\r\n",
    "        assert P.shape[0] == P.shape[1]\r\n",
    "\r\n",
    "        N = P.shape[0]\r\n",
    "        max_thresh = 1 + self._epsilon\r\n",
    "        min_thresh = 1 - self._epsilon\r\n",
    "\r\n",
    "        # Initialize r and c, the diagonals of D1 and D2\r\n",
    "        # and warn if the matrix does not have support.\r\n",
    "        r = np.ones((N, 1))\r\n",
    "        pdotr = P.T.dot(r)\r\n",
    "        total_support_warning_str = (\r\n",
    "            \"Matrix P must have total support. \"\r\n",
    "            \"See documentation\"\r\n",
    "        )\r\n",
    "        if not np.all(pdotr != 0):\r\n",
    "            warnings.warn(total_support_warning_str, UserWarning)\r\n",
    "\r\n",
    "        c = 1 / pdotr\r\n",
    "        pdotc = P.dot(c)\r\n",
    "        if not np.all(pdotc != 0):\r\n",
    "            warnings.warn(total_support_warning_str, UserWarning)\r\n",
    "\r\n",
    "        r = 1 / pdotc\r\n",
    "        del pdotr, pdotc\r\n",
    "\r\n",
    "        P_eps = np.copy(P)\r\n",
    "        while np.any(np.sum(P_eps, axis=1) < min_thresh) \\\r\n",
    "                or np.any(np.sum(P_eps, axis=1) > max_thresh) \\\r\n",
    "                or np.any(np.sum(P_eps, axis=0) < min_thresh) \\\r\n",
    "                or np.any(np.sum(P_eps, axis=0) > max_thresh):\r\n",
    "\r\n",
    "            c = 1 / P.T.dot(r)\r\n",
    "            r = 1 / P.dot(c)\r\n",
    "\r\n",
    "            self._D1 = np.diag(np.squeeze(r))\r\n",
    "            self._D2 = np.diag(np.squeeze(c))\r\n",
    "            P_eps = self._D1.dot(P).dot(self._D2)\r\n",
    "\r\n",
    "            self._iterations += 1\r\n",
    "\r\n",
    "            if self._iterations >= self._max_iter:\r\n",
    "                self._stopping_condition = \"max_iter\"\r\n",
    "                break\r\n",
    "\r\n",
    "        if not self._stopping_condition:\r\n",
    "            self._stopping_condition = \"epsilon\"\r\n",
    "\r\n",
    "        self._D1 = np.diag(np.squeeze(r))\r\n",
    "        self._D2 = np.diag(np.squeeze(c))\r\n",
    "        P_eps = self._D1.dot(P).dot(self._D2)\r\n",
    "\r\n",
    "        return P_eps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "sh = SinkhornKnopp()\r\n",
    "mat = np.random.uniform(0, 1, size=(15,15))\r\n",
    "mat"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.77047748, 0.12255091, 0.29305959, 0.40155392, 0.93531142,\n",
       "        0.58960897, 0.089743  , 0.38033475, 0.54087382, 0.04370785,\n",
       "        0.99758221, 0.68192138, 0.82704384, 0.56828631, 0.97440023],\n",
       "       [0.98092976, 0.12752975, 0.37398163, 0.73441382, 0.66602245,\n",
       "        0.01436947, 0.45737407, 0.68147954, 0.88430475, 0.71784137,\n",
       "        0.4528176 , 0.06150887, 0.10395306, 0.99527805, 0.64350764],\n",
       "       [0.45413436, 0.34270935, 0.52220594, 0.25037549, 0.19730073,\n",
       "        0.04870015, 0.74208657, 0.04089297, 0.70621456, 0.22730178,\n",
       "        0.17932379, 0.59211356, 0.70461578, 0.7215873 , 0.27473461],\n",
       "       [0.2507698 , 0.05314883, 0.48538292, 0.93623987, 0.03893189,\n",
       "        0.24950645, 0.11417394, 0.58147414, 0.77541077, 0.90110477,\n",
       "        0.87222485, 0.73209217, 0.90234108, 0.62504812, 0.7592827 ],\n",
       "       [0.14085701, 0.19139359, 0.75797466, 0.71468478, 0.03009083,\n",
       "        0.20164331, 0.36195132, 0.97064605, 0.04649732, 0.21018688,\n",
       "        0.61119584, 0.82771367, 0.74387093, 0.13787133, 0.15813785],\n",
       "       [0.83229301, 0.53149801, 0.27222847, 0.15654416, 0.81647192,\n",
       "        0.72489539, 0.59506774, 0.12677461, 0.89630035, 0.53326815,\n",
       "        0.61048851, 0.07911646, 0.67884753, 0.44787112, 0.55265529],\n",
       "       [0.61579453, 0.10183108, 0.11992989, 0.61042992, 0.60445995,\n",
       "        0.15093154, 0.90759319, 0.16213206, 0.17225686, 0.51251901,\n",
       "        0.43230578, 0.78581196, 0.3815273 , 0.55051927, 0.11338209],\n",
       "       [0.82567116, 0.04376999, 0.91557454, 0.48134085, 0.89665988,\n",
       "        0.32997639, 0.53689728, 0.67017828, 0.15580052, 0.75850373,\n",
       "        0.95483476, 0.11975699, 0.6495852 , 0.90523592, 0.57552724],\n",
       "       [0.18831632, 0.67943261, 0.40877044, 0.03732229, 0.59206696,\n",
       "        0.16940169, 0.98714967, 0.73245409, 0.13420112, 0.68563254,\n",
       "        0.70969193, 0.87802284, 0.04633855, 0.26165203, 0.24342525],\n",
       "       [0.63537517, 0.81434986, 0.93852805, 0.27545644, 0.43311602,\n",
       "        0.68273487, 0.1188335 , 0.98218565, 0.05584616, 0.46457639,\n",
       "        0.29524044, 0.96551778, 0.33473823, 0.09452421, 0.84291793],\n",
       "       [0.19245065, 0.02062975, 0.63553894, 0.82405665, 0.47958558,\n",
       "        0.3038171 , 0.57212672, 0.63512211, 0.70962201, 0.58247356,\n",
       "        0.01230172, 0.15907376, 0.30309032, 0.60848261, 0.73139147],\n",
       "       [0.79563855, 0.64740851, 0.1606037 , 0.39245079, 0.21069111,\n",
       "        0.0145948 , 0.03052291, 0.0630674 , 0.64504936, 0.81389687,\n",
       "        0.14977125, 0.28725538, 0.87855798, 0.50550125, 0.68294687],\n",
       "       [0.59512049, 0.72267386, 0.86758659, 0.03145305, 0.74114505,\n",
       "        0.20407861, 0.05778804, 0.80079063, 0.90679889, 0.79701446,\n",
       "        0.16773132, 0.50077669, 0.67489527, 0.70969658, 0.32075214],\n",
       "       [0.78527361, 0.69184429, 0.93665163, 0.49452406, 0.68681031,\n",
       "        0.08219297, 0.47177869, 0.40589941, 0.6888977 , 0.70111841,\n",
       "        0.11927998, 0.44699623, 0.6862677 , 0.48120822, 0.02601356],\n",
       "       [0.39969345, 0.12083865, 0.93745881, 0.37176708, 0.88533401,\n",
       "        0.76418335, 0.03676371, 0.65483098, 0.37422557, 0.92819901,\n",
       "        0.59820278, 0.4696007 , 0.55243839, 0.92710197, 0.23843012]])"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "res = sh.fit(mat)\r\n",
    "print(sh._iterations)\r\n",
    "res"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.08087882, 0.02049092, 0.03033349, 0.05142852, 0.10369476,\n",
       "        0.12204   , 0.01230698, 0.04313213, 0.0617261 , 0.00435555,\n",
       "        0.12617845, 0.07703239, 0.08494819, 0.05866753, 0.12278618],\n",
       "       [0.11377027, 0.02355985, 0.04276936, 0.10392427, 0.08158409,\n",
       "        0.00328621, 0.06930083, 0.08538936, 0.1115041 , 0.07903661,\n",
       "        0.06328136, 0.00767702, 0.0117972 , 0.11352489, 0.0895946 ],\n",
       "       [0.06773911, 0.08142376, 0.07680482, 0.04556508, 0.03108204,\n",
       "        0.01432348, 0.1446058 , 0.00658967, 0.11452222, 0.03218599,\n",
       "        0.03222958, 0.09504391, 0.10283908, 0.10585217, 0.04919327],\n",
       "       [0.0272189 , 0.00918881, 0.05194831, 0.12398461, 0.004463  ,\n",
       "        0.0533999 , 0.01618968, 0.06818453, 0.09150086, 0.0928495 ,\n",
       "        0.11407369, 0.08551166, 0.09583339, 0.06672132, 0.09893185],\n",
       "       [0.02027748, 0.04388672, 0.10759247, 0.1255265 , 0.00457505,\n",
       "        0.05723777, 0.06807095, 0.15095816, 0.00727715, 0.02872435,\n",
       "        0.10601752, 0.12822714, 0.10478134, 0.01951935, 0.02732805],\n",
       "       [0.08841151, 0.08992995, 0.02851397, 0.02028872, 0.09160086,\n",
       "        0.15183474, 0.08258002, 0.01454873, 0.10351043, 0.05377583,\n",
       "        0.07813969, 0.00904406, 0.07055951, 0.04678875, 0.07047322],\n",
       "       [0.08760716, 0.02307567, 0.01682375, 0.10595583, 0.09082325,\n",
       "        0.04233963, 0.16868289, 0.02491912, 0.0266427 , 0.06921858,\n",
       "        0.07410653, 0.12030581, 0.05311047, 0.07702506, 0.01936357],\n",
       "       [0.08430298, 0.0071184 , 0.09217669, 0.05996168, 0.09669172,\n",
       "        0.06643271, 0.07161485, 0.07392413, 0.01729429, 0.07351949,\n",
       "        0.11746966, 0.01315833, 0.06489671, 0.0908978 , 0.07054057],\n",
       "       [0.02368449, 0.13611083, 0.05069291, 0.00572703, 0.07864529,\n",
       "        0.04201043, 0.16219416, 0.09952144, 0.01834976, 0.0818609 ,\n",
       "        0.10754933, 0.11883549, 0.00570255, 0.03236354, 0.03675184],\n",
       "       [0.06658126, 0.13592613, 0.09697519, 0.03521762, 0.04793489,\n",
       "        0.1410708 , 0.0162681 , 0.11119249, 0.00636228, 0.04621552,\n",
       "        0.03727857, 0.10887954, 0.03432241, 0.00974138, 0.1060338 ],\n",
       "       [0.02505887, 0.00427864, 0.08159731, 0.1309135 , 0.06595291,\n",
       "        0.07800413, 0.09732185, 0.08934277, 0.10045399, 0.07199913,\n",
       "        0.00193005, 0.02228973, 0.03861579, 0.07791948, 0.11432185],\n",
       "       [0.11425897, 0.14808907, 0.02274163, 0.06876144, 0.03195556,\n",
       "        0.00413272, 0.00572633, 0.00978452, 0.10070834, 0.11095651,\n",
       "        0.0259158 , 0.04439222, 0.12345129, 0.07139248, 0.11773312],\n",
       "       [0.06617833, 0.12800395, 0.09512952, 0.00426736, 0.0870442 ,\n",
       "        0.0447478 , 0.00839508, 0.0962033 , 0.1096276 , 0.08413681,\n",
       "        0.02247433, 0.05992654, 0.07343414, 0.07761388, 0.04281715],\n",
       "       [0.09161208, 0.12856129, 0.10774606, 0.07038894, 0.08462414,\n",
       "        0.01890731, 0.07190285, 0.05115761, 0.08737445, 0.07764831,\n",
       "        0.01676722, 0.05611769, 0.07833864, 0.05521036, 0.00364308],\n",
       "       [0.04242975, 0.0204324 , 0.09812671, 0.04815037, 0.09926047,\n",
       "        0.15995738, 0.00509846, 0.07509876, 0.04318917, 0.09353916,\n",
       "        0.07651622, 0.05364588, 0.05738231, 0.09678916, 0.03038378]])"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "print(np.sum(res, axis=1), np.sum(res, axis=0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1.00000998 1.0000764  0.99997218 1.00006148 0.9999322  0.99972499\n",
      " 1.00025883 0.99994672 1.00004345 1.00002224 0.99992801 1.00008741\n",
      " 1.00001301 1.00002716 0.99989593]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "np.allclose(sh._D1 @ mat @ sh._D2, res)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('ohg': conda)"
  },
  "interpreter": {
   "hash": "a973c368015e0ac2b41d464c33143b64f1c4b6010c6cffead0cdd62362b8fcf3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}