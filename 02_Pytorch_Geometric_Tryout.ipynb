{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQNoqR9rbdtP"
      },
      "source": [
        "# Setup of the Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Fm7tuAQZcBg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import GraphConv\n",
        "from torch_geometric.nn import GINConv\n",
        "\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from IPython.display import Javascript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below is required for some reason as it is not present in the pytorch geometric version (but in the docs it is? It is the copy-pasted sourcecode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qSDOTLJdPC8A"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Callable, List\n",
        "from torch_geometric.typing import Adj\n",
        "\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList, Sequential, Linear, BatchNorm1d, ReLU\n",
        "\n",
        "from torch_geometric.nn.conv import GCNConv, SAGEConv, GINConv, GATConv\n",
        "from torch_geometric.nn.models.jumping_knowledge import JumpingKnowledge\n",
        "\n",
        "\n",
        "class BasicGNN(torch.nn.Module):\n",
        "    r\"\"\"An abstract class for implementing basic GNN models.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Size of each hidden and output sample.\n",
        "        num_layers (int): Number of message passing layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last'):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = hidden_channels\n",
        "        if jk == 'cat':\n",
        "            self.out_channels = num_layers * hidden_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "\n",
        "        self.jk = None\n",
        "        if jk != 'last':\n",
        "            self.jk = JumpingKnowledge(jk, hidden_channels, num_layers)\n",
        "\n",
        "        self.norms = None\n",
        "        if norm is not None:\n",
        "            self.norms = ModuleList(\n",
        "                [copy.deepcopy(norm) for _ in range(num_layers)])\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for norm in self.norms or []:\n",
        "            norm.reset_parameters()\n",
        "        if self.jk is not None:\n",
        "            self.jk.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n",
        "        xs: List[Tensor] = []\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index, *args, **kwargs)\n",
        "            if self.norms is not None:\n",
        "                x = self.norms[i](x)\n",
        "            if self.act is not None:\n",
        "                x = self.act(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            if self.jk is not None:\n",
        "                xs.append(x)\n",
        "        return x if self.jk is None else self.jk(xs)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, num_layers={self.num_layers})')\n",
        "\n",
        "\n",
        "class GCN(BasicGNN):\n",
        "    r\"\"\"The Graph Neural Network from the `\"Semi-supervised\n",
        "    Classification with Graph Convolutional Networks\"\n",
        "    <https://arxiv.org/abs/1609.02907>`_ paper, using the\n",
        "    :class:`~torch_geometric.nn.conv.GCNConv` operator for message passing.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.GCNConv`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\n",
        "                 **kwargs):\n",
        "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\n",
        "                         act, norm, jk)\n",
        "\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels, **kwargs))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.convs.append(\n",
        "                GCNConv(hidden_channels, hidden_channels, **kwargs))\n",
        "\n",
        "\n",
        "\n",
        "class GraphSAGE(BasicGNN):\n",
        "    r\"\"\"The Graph Neural Network from the `\"Inductive Representation Learning\n",
        "    on Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper, using the\n",
        "    :class:`~torch_geometric.nn.SAGEConv` operator for message passing.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.SAGEConv`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\n",
        "                 **kwargs):\n",
        "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\n",
        "                         act, norm, jk)\n",
        "\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels, **kwargs))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.convs.append(\n",
        "                SAGEConv(hidden_channels, hidden_channels, **kwargs))\n",
        "\n",
        "\n",
        "\n",
        "class GIN(BasicGNN):\n",
        "    r\"\"\"The Graph Neural Network from the `\"How Powerful are Graph Neural\n",
        "    Networks?\" <https://arxiv.org/abs/1810.00826>`_ paper, using the\n",
        "    :class:`~torch_geometric.nn.GINConv` operator for message passing.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.GINConv`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\n",
        "                 **kwargs):\n",
        "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\n",
        "                         act, norm, jk)\n",
        "\n",
        "        self.convs.append(\n",
        "            GINConv(GIN.MLP(in_channels, hidden_channels), **kwargs))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.convs.append(\n",
        "                GINConv(GIN.MLP(hidden_channels, hidden_channels), **kwargs))\n",
        "\n",
        "    @staticmethod\n",
        "    def MLP(in_channels: int, out_channels: int) -> torch.nn.Module:\n",
        "        return Sequential(\n",
        "            Linear(in_channels, out_channels),\n",
        "            BatchNorm1d(out_channels),\n",
        "            ReLU(inplace=True),\n",
        "            Linear(out_channels, out_channels),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "class GAT(BasicGNN):\n",
        "    r\"\"\"The Graph Neural Network from the `\"Graph Attention Networks\"\n",
        "    <https://arxiv.org/abs/1710.10903>`_ paper, using the\n",
        "    :class:`~torch_geometric.nn.GATConv` operator for message passing.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.GATConv`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\n",
        "                 **kwargs):\n",
        "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\n",
        "                         act, norm, jk)\n",
        "\n",
        "        if 'concat' in kwargs:\n",
        "            del kwargs['concat']\n",
        "\n",
        "        if 'heads' in kwargs:\n",
        "            assert hidden_channels % kwargs['heads'] == 0\n",
        "        out_channels = hidden_channels // kwargs.get('heads', 1)\n",
        "\n",
        "        self.convs.append(\n",
        "            GATConv(in_channels, out_channels, dropout=dropout, **kwargs))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.convs.append(GATConv(hidden_channels, out_channels, **kwargs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCb8Nj_cbjEK"
      },
      "source": [
        "# Data, Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OEhYnFODTHqm"
      },
      "outputs": [],
      "source": [
        "dataset = MoleculeNet(root=\"data/\", name=\"Tox21\")\r\n",
        "\r\n",
        "dataset = dataset[~dataset.data.y[:, 0].isnan()]#bad?\r\n",
        "#TODO: Do filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j11WiUr-PRH_"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "#TODO: Do more involved splitting (Cluster Cross Validation etc... -> CHEMBL like)\n",
        "dataset = dataset.shuffle()\n",
        "train_dataset = dataset[:6500]\n",
        "test_dataset = dataset[6500:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0gZ-l0npPIca"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CN3sRVuaQ88l"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        #self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        #self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        #self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        #self.gin = GIN(dataset.num_node_features, hidden_channels, 3)\n",
        "        self.gat = GAT(dataset.num_node_features, hidden_channels, 3)\n",
        "        self.lin = Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        # x = self.conv1(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv3(x, edge_index)\n",
        "        x = self.gat(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HvhgQoO8Svw4"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         out = model(data.x.float(), data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y[:, 0].long())  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "\n",
        "     indices_list = []\n",
        "     probs1_list = []\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x.float(), data.edge_index, data.batch)  \n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         correct += int((pred == data.y[:,0].long()).sum())  # Check against ground-truth labels.\n",
        "         \n",
        "         indices = data.y[:,0].long().detach().cpu().numpy()\n",
        "         rs = out.detach().cpu().numpy()\n",
        "\n",
        "         probs1 = rs[:, 1]\n",
        "         \n",
        "         #print(indices.shape, probs1.shape, rs.shape, np.ones_like(indices).shape)\n",
        "         indices_list.append(indices)\n",
        "         probs1_list.append(probs1)\n",
        "         \n",
        "     indices = np.concatenate(indices_list)\n",
        "     probs1 = np.concatenate(probs1_list)\n",
        "\n",
        "     return correct / len(loader.dataset), roc_auc_score(indices, probs1) # Derive ratio of correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXOf7q142xR_",
        "outputId": "e7a7a00b-bc22-476e-a840-6899f1fba2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Train Acc: 0.9575, Train ROC-AUC: 0.7928, Test Acc: 0.9569, Test ROC-AUC: 0.7176\n",
            "Epoch: 002, Train Acc: 0.9575, Train ROC-AUC: 0.8079, Test Acc: 0.9569, Test ROC-AUC: 0.7281\n",
            "Epoch: 003, Train Acc: 0.9575, Train ROC-AUC: 0.8117, Test Acc: 0.9569, Test ROC-AUC: 0.7259\n",
            "Epoch: 004, Train Acc: 0.9623, Train ROC-AUC: 0.8082, Test Acc: 0.9516, Test ROC-AUC: 0.7239\n",
            "Epoch: 005, Train Acc: 0.9628, Train ROC-AUC: 0.8118, Test Acc: 0.9542, Test ROC-AUC: 0.7250\n",
            "Epoch: 006, Train Acc: 0.9569, Train ROC-AUC: 0.8055, Test Acc: 0.9556, Test ROC-AUC: 0.7282\n",
            "Epoch: 007, Train Acc: 0.9575, Train ROC-AUC: 0.8100, Test Acc: 0.9569, Test ROC-AUC: 0.7249\n",
            "Epoch: 008, Train Acc: 0.9622, Train ROC-AUC: 0.8096, Test Acc: 0.9529, Test ROC-AUC: 0.7221\n",
            "Epoch: 009, Train Acc: 0.9628, Train ROC-AUC: 0.8117, Test Acc: 0.9529, Test ROC-AUC: 0.7231\n"
          ]
        }
      ],
      "source": [
        "model = GCN(hidden_channels=64)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\r\n",
        "criterion = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "for epoch in range(1, 10):\r\n",
        "    train()\r\n",
        "    train_acc, roc_auc_train = test(train_loader)\r\n",
        "    test_acc, roc_auc_test = test(test_loader)\r\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train ROC-AUC: {roc_auc_train:.4f}, Test Acc: {test_acc:.4f}, Test ROC-AUC: {roc_auc_test:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w0hKSRj21Al"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Prototype with Pipeline.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a973c368015e0ac2b41d464c33143b64f1c4b6010c6cffead0cdd62362b8fcf3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('ohg': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}