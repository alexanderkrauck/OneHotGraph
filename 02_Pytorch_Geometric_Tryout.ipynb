{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQNoqR9rbdtP"
      },
      "source": [
        "# Setup of the Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "0Fm7tuAQZcBg"
      },
      "outputs": [],
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch_geometric.datasets import MoleculeNet\r\n",
        "from torch_geometric.data import DataLoader\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torch_geometric.nn import GCNConv\r\n",
        "from torch_geometric.nn import GINConv\r\n",
        "from torch_geometric import nn as gnn\r\n",
        "\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "import numpy as np\r\n",
        "from time import time\r\n",
        "\r\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: Modify for the server\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = \"cuda\"\r\n",
        "else:\r\n",
        "    device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below is required for some reason as it is not present in the pytorch geometric version (but in the docs it is? It is the copy-pasted sourcecode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "qSDOTLJdPC8A"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Callable, List\n",
        "from torch_geometric.typing import Adj\n",
        "\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList, Sequential, Linear, BatchNorm1d, ReLU\n",
        "\n",
        "from torch_geometric.nn.conv import GCNConv, SAGEConv, GINConv, GATConv\n",
        "from torch_geometric.nn.models.jumping_knowledge import JumpingKnowledge\n",
        "\n",
        "\n",
        "class BasicGNN(torch.nn.Module):\n",
        "    r\"\"\"An abstract class for implementing basic GNN models.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Size of each hidden and output sample.\n",
        "        num_layers (int): Number of message passing layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last'):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = hidden_channels\n",
        "        if jk == 'cat':\n",
        "            self.out_channels = num_layers * hidden_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "\n",
        "        self.jk = None\n",
        "        if jk != 'last':\n",
        "            self.jk = JumpingKnowledge(jk, hidden_channels, num_layers)\n",
        "\n",
        "        self.norms = None\n",
        "        if norm is not None:\n",
        "            self.norms = ModuleList(\n",
        "                [copy.deepcopy(norm) for _ in range(num_layers)])\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for norm in self.norms or []:\n",
        "            norm.reset_parameters()\n",
        "        if self.jk is not None:\n",
        "            self.jk.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n",
        "        xs: List[Tensor] = []\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index, *args, **kwargs)\n",
        "            if self.norms is not None:\n",
        "                x = self.norms[i](x)\n",
        "            if self.act is not None:\n",
        "                x = self.act(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            if self.jk is not None:\n",
        "                xs.append(x)\n",
        "        return x if self.jk is None else self.jk(xs)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, num_layers={self.num_layers})')\n",
        "\n",
        "\n",
        "class GCN(BasicGNN):\n",
        "    r\"\"\"The Graph Neural Network from the `\"Semi-supervised\n",
        "    Classification with Graph Convolutional Networks\"\n",
        "    <https://arxiv.org/abs/1609.02907>`_ paper, using the\n",
        "    :class:`~torch_geometric.nn.conv.GCNConv` operator for message passing.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.GCNConv`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\n",
        "                 **kwargs):\n",
        "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\n",
        "                         act, norm, jk)\n",
        "\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels, **kwargs))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.convs.append(\n",
        "                GCNConv(hidden_channels, hidden_channels, **kwargs))\n",
        "\n",
        "\n",
        "\n",
        "class GraphSAGE(BasicGNN):\n",
        "    r\"\"\"The Graph Neural Network from the `\"Inductive Representation Learning\n",
        "    on Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper, using the\n",
        "    :class:`~torch_geometric.nn.SAGEConv` operator for message passing.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.SAGEConv`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\n",
        "                 **kwargs):\n",
        "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\n",
        "                         act, norm, jk)\n",
        "\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels, **kwargs))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.convs.append(\n",
        "                SAGEConv(hidden_channels, hidden_channels, **kwargs))\n",
        "\n",
        "\n",
        "\n",
        "class GIN(BasicGNN):\n",
        "    r\"\"\"The Graph Neural Network from the `\"How Powerful are Graph Neural\n",
        "    Networks?\" <https://arxiv.org/abs/1810.00826>`_ paper, using the\n",
        "    :class:`~torch_geometric.nn.GINConv` operator for message passing.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.GINConv`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\n",
        "                 **kwargs):\n",
        "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\n",
        "                         act, norm, jk)\n",
        "\n",
        "        self.convs.append(\n",
        "            GINConv(GIN.MLP(in_channels, hidden_channels), **kwargs))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.convs.append(\n",
        "                GINConv(GIN.MLP(hidden_channels, hidden_channels), **kwargs))\n",
        "\n",
        "    @staticmethod\n",
        "    def MLP(in_channels: int, out_channels: int) -> torch.nn.Module:\n",
        "        return Sequential(\n",
        "            Linear(in_channels, out_channels),\n",
        "            BatchNorm1d(out_channels),\n",
        "            ReLU(inplace=True),\n",
        "            Linear(out_channels, out_channels),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "class GAT(BasicGNN):\n",
        "    r\"\"\"The Graph Neural Network from the `\"Graph Attention Networks\"\n",
        "    <https://arxiv.org/abs/1710.10903>`_ paper, using the\n",
        "    :class:`~torch_geometric.nn.GATConv` operator for message passing.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.`)\n",
        "        act (Callable, optional): The non-linear activation function to use.\n",
        "            (default: :meth:`torch.nn.ReLU(inplace=True)`)\n",
        "        norm (torch.nn.Module, optional): The normalization operator to use.\n",
        "            (default: :obj:`None`)\n",
        "        jk (str, optional): The Jumping Knowledge mode\n",
        "            (:obj:`\"last\"`, :obj:`\"cat\"`, :obj:`\"max\"`, :obj:`\"last\"`).\n",
        "            (default: :obj:`\"last\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.GATConv`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, num_layers: int,\n",
        "                 dropout: float = 0.0,\n",
        "                 act: Optional[Callable] = ReLU(inplace=True),\n",
        "                 norm: Optional[torch.nn.Module] = None, jk: str = 'last',\n",
        "                 **kwargs):\n",
        "        super().__init__(in_channels, hidden_channels, num_layers, dropout,\n",
        "                         act, norm, jk)\n",
        "\n",
        "        if 'concat' in kwargs:\n",
        "            del kwargs['concat']\n",
        "\n",
        "        if 'heads' in kwargs:\n",
        "            assert hidden_channels % kwargs['heads'] == 0\n",
        "        out_channels = hidden_channels // kwargs.get('heads', 1)\n",
        "\n",
        "        self.convs.append(\n",
        "            GATConv(in_channels, out_channels, dropout=dropout, **kwargs))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.convs.append(GATConv(hidden_channels, out_channels, **kwargs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCb8Nj_cbjEK"
      },
      "source": [
        "# Data, Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "OEhYnFODTHqm"
      },
      "outputs": [],
      "source": [
        "dataset = MoleculeNet(root=\"data/\", name=\"Tox21\")\r\n",
        "#I assume this is the right order as this is also like so in the raw version of the automatically downloaded Tox21\r\n",
        "dataset_label_names = [\"NR-AR\",\"NR-AR-LBD\",\"NR-AhR\",\"NR-Aromatase\",\"NR-ER\",\"NR-ER-LBD\",\"NR-PPAR-gamma\",\"SR-ARE\",\"SR-ATAD5\",\"SR-HSE\",\"SR-MMP\",\"SR-p53\"]\r\n",
        "\r\n",
        "#TODO: Do filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "j11WiUr-PRH_"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "#TODO: Do more involved splitting (Cluster Cross Validation etc... -> CHEMBL like)\n",
        "dataset = dataset.shuffle()\n",
        "train_dataset = dataset[:6500]\n",
        "test_dataset = dataset[6500:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "0gZ-l0npPIca"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "CN3sRVuaQ88l"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\r\n",
        "    def __init__(self, n_layers, input_dim, hidden_dim, output_dim, dropout, activation):\r\n",
        "        super(MLP, self).__init__()\r\n",
        "\r\n",
        "        if n_layers == 1:\r\n",
        "            modules = [\r\n",
        "                nn.Dropout(p = dropout),\r\n",
        "                nn.Linear(input_dim, output_dim)\r\n",
        "            ]\r\n",
        "        else:\r\n",
        "            modules = [\r\n",
        "                nn.Dropout(p = dropout),\r\n",
        "                nn.Linear(input_dim, hidden_dim)\r\n",
        "            ]\r\n",
        "\r\n",
        "            for i in range(n_layers - 2):\r\n",
        "                modules.extend([\r\n",
        "                    activation,\r\n",
        "                    nn.Dropout(p = dropout),\r\n",
        "                    nn.Linear(hidden_dim, hidden_dim)\r\n",
        "                ])\r\n",
        "            \r\n",
        "            modules.extend([\r\n",
        "                activation,\r\n",
        "                nn.Dropout(p = dropout),\r\n",
        "                nn.Linear(hidden_dim, output_dim)\r\n",
        "            ])\r\n",
        "\r\n",
        "        self.net = nn.Sequential(*modules)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return torch.sigmoid(self.net(x))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class GCN(nn.Module):\r\n",
        "    def __init__(self, n_hidden_channels, n_graph_layers, n_graph_dropout, n_linear_layers, n_linear_dropout):\r\n",
        "        super(GCN, self).__init__()\r\n",
        "        self.gin = GIN(dataset.num_node_features, n_hidden_channels, n_graph_layers, dropout=n_graph_dropout)\r\n",
        "        \r\n",
        "        self.head = MLP(n_linear_layers, n_hidden_channels, n_hidden_channels, dataset.num_classes, n_linear_dropout, nn.ReLU())\r\n",
        "\r\n",
        "    def forward(self, x, edge_index, batch):\r\n",
        "        # 1. Obtain node embeddings \r\n",
        "        x = self.gin(x, edge_index)\r\n",
        "\r\n",
        "        # 2. Readout layer\r\n",
        "        x = gnn.global_mean_pool(x, batch)  # [batch_size, hidden_channels]\r\n",
        "\r\n",
        "        # 3. Apply a final classifier\r\n",
        "        x =  self.head(x)\r\n",
        "        \r\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "HvhgQoO8Svw4"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, loader, epoch, logger: SummaryWriter):\r\n",
        "\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    n_minibatches = len(loader)\r\n",
        "\r\n",
        "    for batch_nr, data in enumerate(loader):\r\n",
        "        x, edge_index, batch = data.x.float().to(device), data.edge_index.to(device), data.batch.to(device)\r\n",
        "\r\n",
        "        out = model(x, edge_index, batch)\r\n",
        "\r\n",
        "        y = data.y.to(device)\r\n",
        "        is_not_nan = ~y.isnan()\r\n",
        "        y = torch.nan_to_num(y, 0.5)\r\n",
        "\r\n",
        "        loss = (F.binary_cross_entropy(out, y, reduction=\"none\") * is_not_nan).mean() #Same as is the DeepTox paper\r\n",
        "\r\n",
        "        optimizer.zero_grad() \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        logger.add_scalar(f\"BCR_MultiTask\", loss.detach().cpu().numpy(), global_step=n_minibatches * (epoch - 1) + batch_nr + 1)\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "def test(model, loader, epoch:int, logger: SummaryWriter, run_type = \"test\"):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    indices_list = [[] for d in range(dataset.num_classes)]\r\n",
        "    probs_list = [[] for d in range(dataset.num_classes)]\r\n",
        "\r\n",
        "    for data in loader:  # Iterate in batches over the training/test dataset.\r\n",
        "        x, edge_index, batch = data.x.float().to(device), data.edge_index.to(device), data.batch.to(device)\r\n",
        "\r\n",
        "        out = model(x, edge_index, batch)\r\n",
        "\r\n",
        "        for i in range(dataset.num_classes):\r\n",
        "            y = data.y[:,i]\r\n",
        "            is_not_nan = ~y.isnan()\r\n",
        "\r\n",
        "            indices = y[is_not_nan].long().detach().cpu().numpy()\r\n",
        "            rs = out[is_not_nan].detach().cpu().numpy()\r\n",
        "            probs = rs[:, i]\r\n",
        "        \r\n",
        "            #print(indices.shape, probs1.shape, rs.shape, np.ones_like(indices).shape)\r\n",
        "            indices_list[i].append(indices)\r\n",
        "            probs_list[i].append(probs)\r\n",
        "        \r\n",
        "    for i, indices, probs in zip(range(dataset.num_classes), indices_list, probs_list):\r\n",
        "        indices = np.concatenate(indices)\r\n",
        "        probs = np.concatenate(probs)\r\n",
        "        logger.add_scalar(f\"AUC-ROC/{run_type}/{dataset_label_names[i]}\", roc_auc_score(indices, probs), global_step=epoch)\r\n",
        "\r\n",
        "    return 0.5, 0.5 # Derive ratio of correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_config(\r\n",
        "    hidden_channels = 128,\r\n",
        "    head_depth = 3, \r\n",
        "    base_depth = 5, \r\n",
        "    base_dropout = 0.5, \r\n",
        "    head_dropout = 0.5, \r\n",
        "    lr = 1e-2, \r\n",
        "    epochs = 100, \r\n",
        "    config_comment = \"\"\r\n",
        "    ):\r\n",
        "\r\n",
        "    model = GCN(\r\n",
        "        n_hidden_channels = hidden_channels,\r\n",
        "        n_graph_layers = base_depth, \r\n",
        "        n_graph_dropout = base_dropout, \r\n",
        "        n_linear_layers = head_depth, \r\n",
        "        n_linear_dropout = head_dropout\r\n",
        "    ).to(device)\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "    logger = SummaryWriter(comment = config_comment)\r\n",
        "\r\n",
        "    test(model, train_loader, 0, logger, run_type=\"train\")\r\n",
        "    test(model, test_loader, 0, logger, run_type=\"validation\")\r\n",
        "\r\n",
        "    for epoch in range(1, epochs + 1):\r\n",
        "    \r\n",
        "        train(model, optimizer, train_loader, epoch, logger)\r\n",
        "        test(model, train_loader, epoch, logger, run_type=\"train\")\r\n",
        "        test(model, test_loader, epoch, logger, run_type=\"validation\")\r\n",
        "\r\n",
        "        logger.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup Simple GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\r\n",
        "\r\n",
        "def dict_product(dicts):\r\n",
        "\r\n",
        "    return (dict(zip(dicts, x)) for x in itertools.product(*dicts.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_grid = {\r\n",
        "    \"hidden_channels\": [64, 256, 1028],\r\n",
        "    \"head_depth\": [1,2,3,4],\r\n",
        "    \"base_depth\": [3,5,10],\r\n",
        "    \"base_dropout\": [0.5, 0.2],\r\n",
        "    \"head_dropout\": [0.5, 0.2],\r\n",
        "    \"lr\": [1e-2, 1e-3]\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_configs(search_grid, randomly_try_n = -1):\r\n",
        "\r\n",
        "    configurations = [config for config in dict_product(search_grid)]\r\n",
        "    print(f\"Total number of Grid-Search configurations: {len(configurations)}\")\r\n",
        "\r\n",
        "    if randomly_try_n == -1:\r\n",
        "        do_indices = range(len(configurations))\r\n",
        "    else:\r\n",
        "        do_indices = np.random.choice(len(configurations), size=randomly_try_n)\r\n",
        "    \r\n",
        "    print(f\"Number of configurations now being trained {len(do_indices)}\")\r\n",
        "    print(\"--------------------------------------------------------------------------------------------\\n\")\r\n",
        "    \r\n",
        "    for idx in do_indices:\r\n",
        "        \r\n",
        "        config = configurations[idx]\r\n",
        "\r\n",
        "        config_str = str(config).replace(\"'\",\"\").replace(\":\", \"-\").replace(\" \", \"\").replace(\"}\", \"\").replace(\"_\",\"\").replace(\",\", \"_\").replace(\"{\",\"_\")\r\n",
        "\r\n",
        "        print(f\"Training config {config_str} ... \", end=\"\")\r\n",
        "        dt = time()\r\n",
        "    \r\n",
        "        train_config(\r\n",
        "            hidden_channels = config[\"hidden_channels\"], \r\n",
        "            head_depth = config[\"head_depth\"], \r\n",
        "            base_depth =  config[\"base_depth\"], \r\n",
        "            base_dropout =  config[\"base_dropout\"], \r\n",
        "            head_dropout =  config[\"head_dropout\"], \r\n",
        "            lr =  config[\"lr\"], \r\n",
        "            config_comment = config_str\r\n",
        "            )\r\n",
        "            \r\n",
        "        print(f\"done (took {time() - dt:.2f}s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of Grid-Search configurations: 288\n",
            "done (took 308.88s)\n",
            "Doing config _hiddenchannels-64_headdepth-2_basedepth-3_basedropout-0.5_headdropout-0.2_lr-0.001 ... "
          ]
        }
      ],
      "source": [
        "search_configs(search_grid, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Prototype with Pipeline.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a973c368015e0ac2b41d464c33143b64f1c4b6010c6cffead0cdd62362b8fcf3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('ohg': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}